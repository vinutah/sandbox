{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for bentley: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.model_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b63a2644b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn.model_selection'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice splitting up a multi-class into multiple binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = pd.DataFrame({'T': [1, 2, 3, 4, 5]})\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.get_dummies(dat, columns='T')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/flit-train.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I now choose not to train on the name of the test\n",
    "#train_res = pd.get_dummies(train, columns=['compiler', 'name', 'precision'])\n",
    "train_res = pd.get_dummies(train, columns=['compiler', 'precision'])\n",
    "train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(train_res['score0'])\n",
    "data = train_res.drop('score0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This did not split on boundaries of tests well.\n",
    "# Meaning the training set and test set both had\n",
    "# instances of the same FLiT test name.  We want\n",
    "# it to generalize to new tests never before seen.\n",
    "#train_dat, test_dat, train_lab, test_lab = \\\n",
    "#    train_test_split(data, labels, train_size=0.8, random_state=0)\n",
    "names = set(data['name'])\n",
    "train_names = random.sample(names, int(0.8 * len(names)))\n",
    "train_selector = data.name.apply(lambda x: x in train_names)\n",
    "data = data.drop('name', 1)\n",
    "train_dat = data[train_selector]\n",
    "train_lab = labels[train_selector]\n",
    "test_selector = train_selector.apply(lambda x: not x)\n",
    "test_dat = data[test_selector]\n",
    "test_lab = labels[test_selector]\n",
    "print(len(train_dat), len(test_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(train_lab.values.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Train Percent of ones: ',\n",
    "      sum(train_lab.values.ravel()) / len(train_lab))\n",
    "print('Test  Percent of ones: ',\n",
    "      sum(test_lab.values.ravel()) / len(test_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(train_dat, train_lab.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.score(test_dat, test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = clf.predict(test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of affirmative predictions: ', sum(test_pred))\n",
    "print('Size of the test set:              ', len(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred = clf.predict(train_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of affirmative predictions: ', sum(train_pred))\n",
    "print('Size of the train set:             ', len(train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 0.4\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "true_pos = lambda p, l: sum((p == 1) & (l == 1))\n",
    "true_neg = lambda p, l: sum((p == 0) & (l == 0))\n",
    "false_pos = lambda p, l: sum((p == 1) & (l == 0))\n",
    "false_neg = lambda p, l: sum((p == 0) & (l == 1))\n",
    "\n",
    "def vals(p, l):\n",
    "    v = np.asarray([\n",
    "            true_pos(p, l),\n",
    "            false_neg(p, l),\n",
    "            true_neg(p, l),\n",
    "            false_pos(p, l),\n",
    "            ], dtype=np.double)\n",
    "    # Normalize what should be positive examples\n",
    "    v[:2] /= sum(v[:2])\n",
    "    # Normalize what should be negative examples\n",
    "    v[2:] /= sum(v[2:])\n",
    "    return v\n",
    "\n",
    "train_vals = vals(train_pred, train_lab.score0)\n",
    "test_vals = vals(test_pred, test_lab.score0)\n",
    "\n",
    "ind = np.arange(4)\n",
    "width = 0.3\n",
    "train_bar = ax.bar(ind, train_vals*100, width, color='#2b8cbe')\n",
    "test_bar = ax.bar(ind + width, test_vals*100, width, color='#a6bddb')\n",
    "\n",
    "ax.set_ylabel('Percent')\n",
    "ax.set_title('Accuracy of default SVM on FLiT data')\n",
    "ax.set_xticks(ind + width/2)\n",
    "ax.set_xticklabels(('True Pos', 'False Neg', 'True Neg', 'False Pos'))\n",
    "ax.legend((train_bar[0], test_bar[0]), ('Train', 'Test'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty bad.  The SVM simply learned to classify EVERYTHING as positive examples.\n",
    "\n",
    "Let's try something.  Maybe I can duplicate the negative examples so that it's more than 7% of the training set.\n",
    "\n",
    "Another way to approach this would be to change the scoring function.  We would choose not necessarily to optimize the total precision, but rather we want to optimize the average of the separate class precisions.  I don't know how I would do this in sklearn, so I'll go with my first idea for now.  \n",
    "\n",
    "Note: I could try to do this if I were to implement it myself instead of using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[i for i in range(len(train_lab)) if train_lab[i] == 0]\n",
    "#train_lab[train_lab.score0 == 1]\n",
    "print(sum(train_lab.values.ravel()), len(train_lab.values.ravel()))\n",
    "print(sum(test_lab.values.ravel()), len(test_lab.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_zero_dat = train_dat[train_lab.score0 == 0]\n",
    "only_zero_lab = train_lab[train_lab.score0 == 0]\n",
    "only_zero_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_dat = train_dat\n",
    "big_lab = train_lab\n",
    "for i in range(10):\n",
    "    big_dat = pd.concat([big_dat, only_zero_dat])\n",
    "    big_lab = pd.concat([big_lab, only_zero_lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(big_lab), sum(big_lab.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_clf = svm.SVC()\n",
    "big_clf.fit(big_dat, big_lab.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_clf.score(big_dat, big_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_pred = big_clf.predict(big_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sum(big_pred), '/', len(big_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_clf.score(test_dat, test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_clf.score(train_dat, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred_from_big = big_clf.predict(train_dat)\n",
    "print(sum(train_pred_from_big), '/', len(train_pred_from_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred_from_big = big_clf.predict(test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sum(test_pred_from_big), '/', len(test_pred_from_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sum(test_lab.values.ravel()), '/', len(test_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum((train_pred_from_big == 1) & (train_lab.score0 == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 0.4\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "true_pos = lambda p, l: sum((p == 1) & (l == 1))\n",
    "true_neg = lambda p, l: sum((p == 0) & (l == 0))\n",
    "false_pos = lambda p, l: sum((p == 1) & (l == 0))\n",
    "false_neg = lambda p, l: sum((p == 0) & (l == 1))\n",
    "\n",
    "def vals(p, l):\n",
    "    v = np.asarray([\n",
    "            true_pos(p, l),\n",
    "            false_neg(p, l),\n",
    "            true_neg(p, l),\n",
    "            false_pos(p, l),\n",
    "            ], dtype=np.double)\n",
    "    # Normalize what should be positive examples\n",
    "    v[:2] /= sum(v[:2])\n",
    "    # Normalize what should be negative examples\n",
    "    v[2:] /= sum(v[2:])\n",
    "    return v\n",
    "\n",
    "big_vals = vals(big_pred, big_lab.score0)\n",
    "train_vals = vals(train_pred_from_big, train_lab.score0)\n",
    "test_vals = vals(test_pred_from_big, test_lab.score0)\n",
    "\n",
    "ind = np.arange(4)\n",
    "width = 0.3\n",
    "train_bar = ax.bar(ind, train_vals*100, width, color='#2b8cbe')\n",
    "test_bar = ax.bar(ind + width, test_vals*100, width, color='#a6bddb')\n",
    "\n",
    "ax.set_ylabel('Percent')\n",
    "ax.set_title('Accuracy of default SVM on FLiT data')\n",
    "ax.set_xticks(ind + width/2)\n",
    "ax.set_xticklabels(('True Pos', 'False Neg', 'True Neg', 'False Pos'))\n",
    "ax.legend((train_bar[0], test_bar[0]), ('Train', 'Test'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The counts for the examples that are supposed to be positive (i.e. 'True Pos' and 'False Neg') are normalized separately from the examples that should be negative (i.e. 'True Neg' and 'False Pos').  I wanted to get percentages of accuracy in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(big_vals)\n",
    "print(train_vals)\n",
    "print(test_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder what the classifier found to be good features to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Find the coefficients of the SVM to see what it learned\n",
    "\n",
    "**TODO:** Do cross-validation with different kernels and different C values\n",
    "\n",
    "There is code here: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\n",
    "\n",
    "This code does some cross validation using the sklearn module.  It would be good to become much more familiar with this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_lab, test_pred_from_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
